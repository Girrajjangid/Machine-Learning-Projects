{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings as w\n",
    "w.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train = pd.read_csv('../../DataSets/NumericCategorical/TrainingData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_test = pd.read_csv('../../DataSets/NumericCategorical/TestData.csv')\n",
    "raw_data_submission_format = pd.read_csv('../../DataSets/NumericCategorical/SubmissionFormat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.shape   # (18) Features + (9) Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first we have to prepare our training and testing dataset with similar to submission format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_submission_format.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It contains probability of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_submission_format.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_col = raw_data_submission_format.columns.tolist()\n",
    "submission_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating labels and add first column to make order in correct way\n",
    "train_labels = pd.DataFrame(raw_data_train['Unnamed: 0']) \n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# create TrainLabels #\n",
    "######################\n",
    "for col in submission_col:\n",
    "    if col == 'Unnamed: 0': \n",
    "        continue\n",
    "    parts = col.split('__')\n",
    "    train_labels[col] = np.where(raw_data_train[parts[0]] == parts[1] , 1 , 0) # return 1 \n",
    "        #where parts[1] == raw_data_train[parts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(raw_data_train.isna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data_train.columns.shape) # 15 + (1 unnamed + 1 ID + 1 total) features  9 labels\n",
    "print(raw_data_test.columns.shape)  # 15 + (1 unnamed + 1 ID + 1 total) features  11 unnamed (all empty)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels.shape)\n",
    "train_labels.head() # One hot encoded labels \n",
    "# 9 labels with 104 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trainlabels is local disk\n",
    "#train_labels.to_csv('train_labels.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### create  Train_data ####\n",
    "###########################\n",
    "# this file contains only features which are given during the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the labels in dataset each contains different categories\n",
    "del raw_data_train['Function']\n",
    "del raw_data_train['Use']\n",
    "del raw_data_train['Sharing']\n",
    "del raw_data_train['Reporting']\n",
    "del raw_data_train['Student_Type']\n",
    "del raw_data_train['Position_Type']\n",
    "del raw_data_train['Object_Type']\n",
    "del raw_data_train['Pre_K']\n",
    "del raw_data_train['Operating_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.head()  # Job_Title_Description contains words with 'comma' \n",
    "# We need to remove them to make a 'Bag of words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can see that only unnamed: 0 (actually it is ID) contains\n",
    "# all 4,00,277 non null values other contains not all values\n",
    "\n",
    "# object represents text data type and int,float represent numerical datatype\n",
    "# Unnamed: 26 is empty column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 'comma's' and quotes\n",
    "for column in raw_data_train.columns.tolist():\n",
    "    if raw_data_train[column].dtype == 'object':\n",
    "        raw_data_train[column] = raw_data_train[column].str.replace(',' , '')\n",
    "        raw_data_train[column] = raw_data_train[column].str.replace('\"' , '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(raw_data_train.isnull())\n",
    "plt.show() # light pink indicates null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_data_train['Unnamed: 26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.isna().sum() # Now it's better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_train.to_csv('train_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Create test data file ##\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data_train.columns.shape) # 15 + (1 unnamed + 1 ID + 1 total) features  9 labels\n",
    "print(raw_data_test.columns.shape)  # 15 + (1 unnamed + 1 ID + 1 total) features  11 unnamed (all empty)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in raw_data_test.columns:\n",
    "    if i in raw_data_train.columns:\n",
    "        continue\n",
    "    else:\n",
    "        l.append(i)\n",
    "        print(i)  # these columns are not in train data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of columns  must be same \n",
    "train_col_order = raw_data_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in raw_data_test.columns.tolist():\n",
    "    if raw_data_test[column].dtype == 'object':\n",
    "        raw_data_test[column] = raw_data_test[column].str.replace(',',' ')\n",
    "        raw_data_test[column] = raw_data_test[column].str.replace('\"','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_test[train_col_order].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data_test[train_col_order].to_csv('test_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.iloc[:1000,:].to_csv('train_data.csv',index = False)\n",
    "#test_data.iloc[:1000,:].to_csv('test_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to create our Online Model for Semi-Structured Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from math import log , exp, sqrt\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if you use console screen to get input from user\n",
    "\n",
    "# what we want from console python.py epochs probability_threshold\n",
    "# trainprediction.py  4  0.5\n",
    "#  if len(sys.argv) != 3:  \n",
    "#   print('Usage: pypy Online.py <epochs> <use_example_probability>')\n",
    "#   print('epochs is number of passes over the training data')\n",
    "#   print('use_example_probability is the probability of using an example in an epoch')\n",
    "#   sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = int(sys.argv[1])\n",
    "#print(\"Number of epochs:\",epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is a cheap way to add randomness to the order of training examples\n",
    "# but use with caution as it does not guarantee all training examples will be seen.\n",
    "# Use 1 if you want to train in order examples appear in the file\n",
    "# use_example_probability = float(sys.argv[2]) \n",
    "#print (\"Use Example Probability:\",use_example_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which original features to keep and discard in the model\n",
    "#####################################\n",
    "## Feature_name  ##  columns_index ##\n",
    "#####################################\n",
    "# ID's                   =   00     #\n",
    "# Object_Description     =   01     #\n",
    "# Text_2                 =   02     #\n",
    "# SubFund_Description    =   03     #\n",
    "# Job_Title_Description  =   04     #\n",
    "# Text_3                 =   05     #\n",
    "# Text_4                 =   06     #\n",
    "# Sub_Object_Description =   07     #\n",
    "# Location_Description   =   08     #\n",
    "# FTE                    =   09     #\n",
    "# Function_Description   =   10     #\n",
    "# Facility_or_Department =   11     #\n",
    "# Position_Extra         =   12     #\n",
    "# Total                  =   13     #\n",
    "# Program_Description    =   14     #\n",
    "# Fund_Description       =   15     #\n",
    "# Text_1                 =   16     #\n",
    "#####################################\n",
    "\n",
    "# We have to prepare a script in which we just pass test_data with noise and scipt need to take only these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now restarting kernel so all other variables are delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'train_data.csv'  # path to training file\n",
    "label = 'train_label.csv'  # path to label file of training data\n",
    "test  = 'test_data.csv'  # path to testing file\n",
    "\n",
    "epochs = 4\n",
    "use_example_probability = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data  = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 17)\n",
      "(1000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(train_data.isna())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in train_data.columns.tolist():\n",
    "    d[i] = len(train_data[i].unique())\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(d.items(), key = lambda kv : kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing import features\n",
    "originals = list(range(17))\n",
    "originals  # all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Found that removing 5 : Index (Text_3)  and 7 : index (Sub Object Description) generaly helped\n",
    "originals.remove(5)\n",
    "originals.remove(7)\n",
    "originals # new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction pairs and triples\n",
    "pairs = [[1,2,3,4],[6,8],[4,12],[1,4,8,10]]\n",
    "triples = [[1,4,12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs [[1, 2, 3, 4], [6, 8], [4, 12], [1, 4, 8, 10]]\n",
      "triples [[1, 4, 12]]\n"
     ]
    }
   ],
   "source": [
    "print('pairs',pairs)\n",
    "print('triples',triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = 2**18 #b number of weights use for each model, we have 104 of them\n",
    "alpha = 0.10 # learning rate for sgd optimization\n",
    "print(D,alpha)\n",
    "epochs , use_example_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities ############################################\n",
    "\n",
    "# Used for assigning the number feat to a categorical level 0 to N\n",
    "# INPUT:\n",
    "#     feat: the numerical predictor\n",
    "#     b: list representing the boundaries for bins\n",
    "# OUTPUT:\n",
    "#     a categorical level 0 to N\n",
    "def boundary(feat,b):\n",
    "    f = float(feat)\n",
    "    s = 0\n",
    "    for step in b:\n",
    "        if f < step:\n",
    "            return s\n",
    "        s += 1\n",
    "    return s\n",
    "\n",
    "# Our hashing function\n",
    "# INPUT:\n",
    "#     s: the string or number\n",
    "# OUTPUT:\n",
    "#     an integer between 0 and D-1\n",
    "def hash_it(s):\n",
    "    return abs(hash(s)) % D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "###  function, generator definition  ###\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 =====> 332\n",
      "0.11349913633333333 =====> 176\n",
      "0.22699827266666667 =====> 162\n",
      "0.34049740900000003 =====> 158\n",
      "0.45399654533333333 =====> 154\n",
      "0.5674956816666666 =====> 140\n",
      "0.6809948180000001 =====> 132\n",
      "0.7944939543333334 =====> 125\n",
      "0.9079930906666667 =====> 118\n",
      "1.021492227 =====> 1\n",
      "\n",
      "0.0 =====> 332\n",
      "0.00431 =====> 245\n",
      "0.131 =====> 172\n",
      "0.911 =====> 118\n",
      "1 =====> 110\n",
      "50 =====> 0\n"
     ]
    }
   ],
   "source": [
    "# spliting the data into these range's  for FTE ( 9th index )\n",
    "for i in (np.linspace(train_data.FTE.min() , train_data.FTE.max(),10)):\n",
    "    print(i,'=====>',train_data.FTE.between(i,i+1).sum())\n",
    "    \n",
    "print()\n",
    "\n",
    "for i in [0.0,0.00431,0.131,0.911,1,50]:\n",
    "    print(i,'=====>' ,train_data.FTE.between(i,i+1).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b13 = [-706.968,-8.879,\n",
    "    7.85,41.972,\n",
    "    73.798,109.55,\n",
    "    160.786,219.736,\n",
    "    318.619,461.23,\n",
    "    646.73,938.36,\n",
    "    1317.584,2132.933,\n",
    "    3652.662,6659.524,\n",
    "    18551.459,39754.287,\n",
    "    64813.342,129700000]\n",
    "b9 = [0.0,0.00431,0.131,0.911,1,50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     bounded logarithmic loss of p given y\n",
    "\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "\n",
    "def predict(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 100.), -100.)))  # bounded sigmoid\n",
    "\n",
    "# D. Update given model\n",
    "# INPUT:\n",
    "# alpha: learning rate\n",
    "#     w: weights\n",
    "#     n: sum of previous absolute gradients for a given feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature, a list of indices\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# MODIFIES:\n",
    "#     w: weights\n",
    "#     n: sum of past absolute gradients\n",
    "def update(alpha, w, n, x, p, y,k):\n",
    "    for i in x:\n",
    "        # alpha / sqrt(n) is the adaptive learning rate\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1.\n",
    "        n[i] += abs(p - y)\n",
    "        w[i] = w[i] - ((p - y) * 1. ) * alpha / n[i] ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function, generator definitions ############################################\n",
    "\n",
    "# A. x, y generator\n",
    "# This is where:\n",
    "# * All the feature hashes are generated\n",
    "# * All feature engineering happens\n",
    "# INPUT:\n",
    "#     path: path to TrainPredictors.csv or TestData2.csv\n",
    "#     label_path: (optional) path to TrainLabels.csv\n",
    "# YIELDS:\n",
    "#     ID: id of the instance\n",
    "#     x: list of hashes for predictors\n",
    "#     y: (if label_path is present) binary label\n",
    "def data(path, label_path=None):\n",
    "    # Boundaries for numerical binning of FTE (9) and Total (13)\n",
    "    b13 = [-706.968,-8.879,\n",
    "    7.85,41.972,\n",
    "    73.798,109.55,\n",
    "    160.786,219.736,\n",
    "    318.619,461.23,\n",
    "    646.73,938.36,\n",
    "    1317.584,2132.933,\n",
    "    3652.662,6659.524,\n",
    "    18551.459,39754.287,\n",
    "    64813.342,129700000]\n",
    "\n",
    "    b9 = [0.0,0.00431,0.131,0.911,1,50]\n",
    "\n",
    "    for t, line in enumerate(open(path)):\n",
    "        # Intcercept term\n",
    "        x = [0]\n",
    "\n",
    "        # Skip headers\n",
    "        if t == 0:\n",
    "            if label_path:\n",
    "                label = open(label_path)\n",
    "                label.readline()  # we don't need the headers\n",
    "            continue\n",
    "\n",
    "        # c is an index for the kept original features (15 of them)\n",
    "        # TODO: drop c and use m for hashing, c was kept for reproducibility\n",
    "        # m is the index for all the original features (17 of them)\n",
    "        # feat is the original raw text or value for feature\n",
    "        c =0\n",
    "        for m, feat in enumerate(line.rstrip().split(',')):\n",
    "            # Drop unwanted original features\n",
    "            if m not in originals:\n",
    "                continue\n",
    "\n",
    "            if m == 0:\n",
    "                ID = int(feat)\n",
    "            else:\n",
    "                # convert floats into categorical levels\n",
    "                # variables 9 (FTE) and 13 (Total) are only numericals\n",
    "                if m == 13:\n",
    "                    if feat == \"\": feat = 0\n",
    "                    feat = boundary(feat,b13)\n",
    "                if m == 9:\n",
    "                    if feat == \"\": feat = -3\n",
    "                    feat = boundary(feat,b9)\n",
    "\n",
    "                # Lowercase and trim so hashes match more often\n",
    "                feat = str(feat).strip().lower()\n",
    "\n",
    "                # First we hash the original feature.  For example, if the\n",
    "                # feature is \"special education\" and the original feature index is 4, we\n",
    "                # hash \"4_special education\"\n",
    "\n",
    "                original_feature = str(c) + '_' + feat\n",
    "                x.append( hash_it(original_feature) )\n",
    "\n",
    "                # Next we break up the original feature value into word parts\n",
    "                # i.e. create bag-of-word features here\n",
    "                parts = re.split(' |/|-',feat)\n",
    "\n",
    "                for i in range(len(parts)):\n",
    "                    token = parts[i].strip().lower()\n",
    "                    if token == '': continue\n",
    "\n",
    "                    # First we hash each token along with its original position index\n",
    "                    # For example, for the feature value \"special education\" we hash\n",
    "                    # its tokens as \"4_special\" and \"4_education\" in successive steps of this loop\n",
    "                    positioned_word = str(c) + '_' + token\n",
    "                    x.append( hash_it( positioned_word ) )\n",
    "\n",
    "                    # Next we hash each token by itself, ignoring any information about its position\n",
    "                    # For example, for \"special education\" we hash \"special\" and \"education\"\n",
    "                    # regardless of what index position the original feature appeared in.\n",
    "                    # This views all the feature values in an example as making up a single document\n",
    "                    x.append( hash_it( token ) )\n",
    "\n",
    "                c = c + 1\n",
    "\n",
    "        # Up to this point we've been breaking original features down into smaller features\n",
    "        # Now we level up and compose original features with each other into larger interction features\n",
    "\n",
    "        row = line.rstrip().split(',')\n",
    "\n",
    "        # Start with pairs.  Make pairs from interaction groups defined in pairs variable.\n",
    "        for interactions in pairs:\n",
    "            for i in range(len(interactions)):\n",
    "                for j in range(i+1,len(interactions)):\n",
    "                    pair = row[interactions[i]]+\"_x_\"+row[interactions[j]]\n",
    "                    x.append( hash_it(pair) )\n",
    "\n",
    "        # Do the same thing for triples\n",
    "        for triple in triples:\n",
    "            trip = row[triple[0]]+\"_x_\"+row[triple[1]] + '_x_' +row[triple[2]]\n",
    "            x.append( hash_it(trip) )\n",
    "\n",
    "        if label_path:\n",
    "            y = [float(y) for y in label.readline().split(',')[1:]]\n",
    "\n",
    "        yield (ID, x, y) if label_path else (ID, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing #######################################################\n",
    "start = datetime.now()\n",
    "# Number of models.\n",
    "DIM = 104\n",
    "K = range(DIM)\n",
    "w = [[0.] * D for k in range(DIM)]\n",
    "n = [[0.] * D for k in range(DIM)]\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "loss = 0.\n",
    "rec = 0\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    for ID, x, y in data(train,label):\n",
    "        # Randomly choose whether or not to train with this example in this epoch\n",
    "        if random.random() > use_example_probability: continue\n",
    "        # record counter\n",
    "        rec += 1\n",
    "        # get predictions and train on all labels\n",
    "        for k in K:\n",
    "            p = predict(x, w[k])\n",
    "            update(alpha, w[k], n[k], x, p, y[k],k)\n",
    "            loss += logloss(p, y[k])  # for progressive validation\n",
    "\n",
    "        # print out progress, so that we know everything is working\n",
    "        if rec % 50000 == 0:\n",
    "            print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "                datetime.now(), rec, (loss/float(DIM))/rec))\n",
    "#h = ',Function__Aides Compensation,Function__Career & Academic Counseling,\n",
    "#Function__Communications,Function__Curriculum Development,Function__Data Processing & Information Services,\n",
    "#Function__Development & Fundraising,Function__Enrichment,Function__Extended Time & Tutoring,Function__Facilities & Maintenance,\n",
    "#Function__Facilities Planning,\"Function__Finance, Budget, Purchasing & Distribution\",Function__Food Services,Function__Governance,\n",
    "#Function__Human Resources,Function__Instructional Materials & Supplies,Function__Insurance,Function__Legal,\n",
    "#Function__Library & Media,Function__NO_LABEL,Function__Other Compensation,Function__Other Non-Compensation,\n",
    "#Function__Parent & Community Relations,Function__Physical Health & Services,Function__Professional Development,Function__Recruitment,\n",
    "#Function__Research & Accountability,Function__School Administration,Function__School Supervision,Function__Security & Safety,\n",
    "#Function__Social & Emotional,Function__Special Population Program Management & Support,\n",
    "#Function__Student Assignment,Function__Student Transportation,Function__Substitute Compensation,\n",
    "#Function__Teacher Compensation,Function__Untracked Budget Set-Aside,Function__Utilities,\n",
    "#Object_Type__Base Salary/Compensation,Object_Type__Benefits,Object_Type__Contracted Services,Object_Type__Equipment & Equipment Lease,Object_Type__NO_LABEL,Object_Type__Other Compensation/Stipend,Object_Type__Other Non-Compensation,Object_Type__Rent/Utilities,Object_Type__Substitute Compensation,Object_Type__Supplies/Materials,Object_Type__Travel & Conferences,Operating_Status__Non-Operating,\"Operating_Status__Operating, Not PreK-12\",Operating_Status__PreK-12 Operating,Position_Type__(Exec) Director,Position_Type__Area Officers,Position_Type__Club Advisor/Coach,Position_Type__Coordinator/Manager,Position_Type__Custodian,Position_Type__Guidance Counselor,Position_Type__Instructional Coach,Position_Type__Librarian,Position_Type__NO_LABEL,Position_Type__Non-Position,Position_Type__Nurse,Position_Type__Nurse Aide,Position_Type__Occupational Therapist,Position_Type__Other,Position_Type__Physical Therapist,Position_Type__Principal,Position_Type__Psychologist,Position_Type__School Monitor/Security,Position_Type__Sec/Clerk/Other Admin,Position_Type__Social Worker,Position_Type__Speech Therapist,Position_Type__Substitute,Position_Type__TA,Position_Type__Teacher,Position_Type__Vice Principal,Pre_K__NO_LABEL,Pre_K__Non PreK,Pre_K__PreK,Reporting__NO_LABEL,Reporting__Non-School,Reporting__School,Sharing__Leadership & Management,Sharing__NO_LABEL,Sharing__School Reported,Sharing__School on Central Budgets,Sharing__Shared Services,Student_Type__Alternative,Student_Type__At Risk,Student_Type__ELL,Student_Type__Gifted,Student_Type__NO_LABEL,Student_Type__Poverty,Student_Type__PreK,Student_Type__Special Education,Student_Type__Unspecified,Use__Business Services,Use__ISPD,Use__Instruction,Use__Leadership,Use__NO_LABEL,Use__O&M,Use__Pupil Services & Enrichment,Use__Untracked Budget Set-Aside'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict(x,w[103]) # weight of all 103 columns\n",
    "#len(x) # 72 hashes\n",
    "#len(w[0]) #262144 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out weights\n",
    "print('writing weights to file')\n",
    "with open('weights.pkl', 'w') as f:\n",
    "    pickle.dump(w, f)\n",
    "\n",
    "output = './submission1234.csv'\n",
    "\n",
    "with open(output, 'w') as outfile:\n",
    "    outfile.write(h + '\\n')\n",
    "    for ID, x in data(test):\n",
    "        outfile.write(str(ID))\n",
    "        for k in K:\n",
    "            p = predict(x, w[k])\n",
    "            outfile.write(',%s' % str(p))\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print('Done, elapsed time: %s' % str(datetime.now() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
