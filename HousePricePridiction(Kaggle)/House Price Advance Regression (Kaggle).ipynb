{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import numpy       as np\n",
    "import pandas      as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn     as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1379, 146), (1379,), (1459, 146))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Datasets\n",
    "raw_data = pd.read_csv('F:/Books/Machine Learning/DataSets/house-prices-advanced-regression-technique/train.csv') \n",
    "test_data = pd.read_csv('F:/Books/Machine Learning/DataSets/house-prices-advanced-regression-technique/test.csv')\n",
    "\n",
    "# checking total no. of object,int,float type features\n",
    "object_features = raw_data.dtypes[raw_data.dtypes == 'object'].keys()\n",
    "int_features    = raw_data.dtypes[raw_data.dtypes == 'int64'].keys()\n",
    "float_features  = raw_data.dtypes[raw_data.dtypes == 'float64'].keys()\n",
    "\n",
    "\n",
    "outliers = []\n",
    "drop_int_feat = []\n",
    "outliers.extend(raw_data.LotArea[raw_data.LotArea > 30000].index.tolist()) \n",
    "outliers.extend(raw_data.OverallQual[raw_data.OverallQual < 3].index.tolist()) \n",
    "outliers.extend(raw_data.OverallCond[raw_data.OverallCond < 3].index.tolist())  \n",
    "outliers.extend(raw_data.BsmtFinSF1[raw_data.BsmtFinSF1 > 2000].index.tolist()) \n",
    "outliers.extend(raw_data.LowQualFinSF[raw_data.LowQualFinSF > 500].index.tolist()) \n",
    "outliers.extend(raw_data.BsmtFullBath[raw_data.BsmtFullBath > 2].index.tolist())\n",
    "outliers.extend(raw_data.TotalBsmtSF[raw_data.TotalBsmtSF > 2600].index.tolist())\n",
    "outliers.extend(raw_data['1stFlrSF'][raw_data['1stFlrSF'] > 2650 ].index.tolist())\n",
    "outliers.extend(raw_data['2ndFlrSF'][raw_data['2ndFlrSF'] > 1600 ].index.tolist())\n",
    "outliers.extend(raw_data.BsmtUnfSF[raw_data.BsmtUnfSF > 2200].index.tolist())\n",
    "outliers.extend(raw_data.FullBath[raw_data.FullBath < 1].index.tolist()) \n",
    "outliers.extend(raw_data.BedroomAbvGr[raw_data.BedroomAbvGr > 6].index.tolist())\n",
    "outliers.extend(raw_data.BedroomAbvGr[raw_data.BedroomAbvGr < 1].index.tolist())\n",
    "outliers.extend(raw_data.KitchenAbvGr[raw_data.KitchenAbvGr > 2].index.tolist())\n",
    "outliers.extend(raw_data.GarageArea[raw_data.GarageArea > 1250].index.tolist())\n",
    "outliers.extend(raw_data.WoodDeckSF[raw_data.WoodDeckSF > 600].index.tolist()) \n",
    "outliers.extend(raw_data.OpenPorchSF[raw_data.OpenPorchSF > 400].index.tolist()) \n",
    "outliers.extend(raw_data.EnclosedPorch[raw_data.EnclosedPorch > 300].index.tolist()) \n",
    "outliers.extend(raw_data.ScreenPorch[raw_data.ScreenPorch > 350].index.tolist())  \n",
    "outliers.extend(raw_data.SalePrice[raw_data.SalePrice > 560000].index.tolist())  \n",
    "\n",
    "drop_int_feat.append('BsmtHalfBath')\n",
    "drop_int_feat.append('BsmtFinSF2')\n",
    "drop_int_feat.append('3SsnPorch')\n",
    "drop_int_feat.append('PoolArea')\n",
    "drop_int_feat.append('MiscVal')\n",
    "\n",
    "\n",
    "# Fill missing values after removing outliers\n",
    "\n",
    "outlier = raw_data.LotFrontage[raw_data.LotFrontage > 200].index.tolist()\n",
    "outliers.extend(outlier)\n",
    "\n",
    "m = raw_data.LotFrontage.drop(outlier).mean()\n",
    "s = raw_data.LotFrontage.drop(outlier).std()\n",
    "randoms = np.random.normal(m , s/1.5 , raw_data.LotFrontage.isna().sum())\n",
    "raw_data.LotFrontage[raw_data.LotFrontage.isna()] = randoms\n",
    "\n",
    "\n",
    "outlier = raw_data.MasVnrArea[raw_data.MasVnrArea > 1100].index.tolist()\n",
    "outliers.extend(outlier)\n",
    "m = raw_data.MasVnrArea.drop(outlier).mean()\n",
    "raw_data.MasVnrArea.fillna(m,inplace=True)\n",
    "\n",
    "\n",
    "x = np.random.normal(round(raw_data.GarageYrBlt.mean()),\n",
    "                        round(raw_data.GarageYrBlt.std()/2),\n",
    "                        raw_data.GarageYrBlt.isna().sum())\n",
    "raw_data.GarageYrBlt[raw_data.GarageYrBlt.isna()]  =  list(map(round,x))\n",
    "\n",
    "from collections import Counter\n",
    "# Detect outlier using Interquartile rate\n",
    "\n",
    "def outlierDetect(df, no_of_feature_contains = 3):\n",
    "    '''pass pandas\"s dataframe'''\n",
    "    outlier_indices = []\n",
    "    for col in df:\n",
    "        q1 = np.percentile(df[col],25)\n",
    "        q3 = np.percentile(df[col],75)\n",
    "        # IQR\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        outlier_index = df[ (df[col] < lower) | (df[col] > upper)].index.tolist()\n",
    "        outlier_indices.extend(outlier_index)\n",
    "\n",
    "    counted_outliers = Counter(outlier_indices)    \n",
    "    indexs = [k for k,v in counted_outliers.items() if v > no_of_feature_contains] \n",
    "    return indexs\n",
    "\n",
    "\n",
    "outliers.extend(outlierDetect(raw_data[int_features],no_of_feature_contains = 5))\n",
    "\n",
    "# Dropping all outliers \n",
    "raw_data.drop(set(outliers), inplace = True)\n",
    "\n",
    "# Seperate labels from dataset\n",
    "raw_label = raw_data.SalePrice\n",
    "raw_data.drop('SalePrice',axis=1,inplace=True)\n",
    "\n",
    "# Droping int features\n",
    "raw_data.drop(drop_int_feat,axis=1,inplace=True)\n",
    "test_data.drop(drop_int_feat,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "int_features = int_features.append(raw_data.select_dtypes('float64').columns)\n",
    "int_features = int_features.drop(drop_int_feat)\n",
    "int_features = int_features.drop('Id')\n",
    "int_features = int_features.drop('SalePrice')\n",
    "\n",
    "#uint8_features = raw_data.select_dtypes('uint8').columns\n",
    "\n",
    "\n",
    "outlier = test_data.LotFrontage[test_data.LotFrontage > 150].index.tolist()\n",
    "m = test_data.LotFrontage.drop(outlier).mean()                                      \n",
    "s = test_data.LotFrontage.drop(outlier).std()\n",
    "randoms = np.random.normal(m , s/1.5 , test_data.LotFrontage.isna().sum())\n",
    "test_data.LotFrontage[test_data.LotFrontage.isna()] = randoms\n",
    "\n",
    "test_data.MasVnrArea.fillna(0,inplace=True)\n",
    "test_data.BsmtFinSF1.fillna(0,inplace=True)\n",
    "test_data.BsmtUnfSF.fillna(0,inplace=True)\n",
    "test_data.TotalBsmtSF.fillna(0,inplace=True)\n",
    "test_data.BsmtFullBath.fillna(0,inplace=True)\n",
    "test_data.GarageCars.fillna(2,inplace=True)\n",
    "test_data.GarageArea.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "x = np.random.normal(round(test_data.GarageYrBlt.mean()),\n",
    "                        round(test_data.GarageYrBlt.std()/2),\n",
    "                        test_data.GarageYrBlt.isna().sum())\n",
    "test_data.GarageYrBlt[test_data.GarageYrBlt.isna()]  =  list(map(round,x))\n",
    "\n",
    "\n",
    "\n",
    "drop_obj_feat = ['Street','Alley','Utilities','LandSlope','Condition2','RoofMatl','Heating',\n",
    "                'BsmtCond','PoolQC','MiscFeature','HeatingQC','Functional','GarageQual','GarageCond',\n",
    "                'PavedDrive','MiscFeature','ExterCond','CentralAir','Fence','FireplaceQu','MasVnrType','Electrical']\n",
    "\n",
    "object_features_2 = object_features.drop(drop_obj_feat)\n",
    "\n",
    "# BsmtQual fillna using its distribution\n",
    "t = [*['TA']*25]\n",
    "t.extend(['Gd']*20)\n",
    "temp = r.sample(t,36)\n",
    "raw_data.BsmtQual[raw_data.BsmtQual.isna()] = temp\n",
    "\n",
    "# BsmtExposure fillna using its distribution\n",
    "raw_data.BsmtExposure[raw_data.BsmtExposure.isna()] = raw_data.BsmtExposure.mode()[0]\n",
    "\n",
    "# BsmtFinType1 fillna using its distribution\n",
    "t = [*['GLQ']*21]\n",
    "t.extend(['Unf']*20)\n",
    "temp = r.sample(t,36)\n",
    "raw_data.BsmtFinType1[raw_data.BsmtFinType1.isna()] = temp\n",
    "\n",
    "# BsmtFinType2 fillna using its distribution\n",
    "raw_data.BsmtFinType2[raw_data.BsmtFinType2.isna()] = raw_data.BsmtFinType2.mode()[0]\n",
    "\n",
    "# Electrical fillna using its distribution\n",
    "raw_data.Electrical[raw_data.Electrical.isna()] = raw_data.Electrical.mode()[0]\n",
    "\n",
    "# GarageType fillna using its distribution\n",
    "t = [*['Attchd']*80]\n",
    "t.extend(['Detchd']*8)\n",
    "temp = r.sample(t,71)\n",
    "raw_data.GarageType[raw_data.GarageType.isna()] = temp\n",
    "\n",
    "# GarageFinish fillna using its distribution\n",
    "t = [*['Unf']*35]\n",
    "t.extend(['RFn']*30)\n",
    "t.extend(['Fin']*25)\n",
    "temp = r.sample(t,71)\n",
    "raw_data.GarageFinish[raw_data.GarageFinish.isna()] = temp\n",
    "\n",
    "# MSZoning fillna using its distribution\n",
    "test_data.MSZoning[test_data.MSZoning.isna()] = test_data.MSZoning.mode()[0]\n",
    "\n",
    "# Exterior1st fillna using its distribution\n",
    "test_data.Exterior1st[test_data.Exterior1st.isna()] = test_data.Exterior1st.mode()[0]\n",
    "\n",
    "# Exterior2nd  fillna using its distribution\n",
    "test_data.Exterior2nd[test_data.Exterior2nd.isna()] = test_data.Exterior2nd.mode()[0]\n",
    "\n",
    "# BsmtQual fillna using its distribution\n",
    "t = [*['TA']*30]\n",
    "t.extend(['Gd']*25)\n",
    "temp = r.sample(t,44)\n",
    "test_data.BsmtQual[test_data.BsmtQual.isna()] = temp\n",
    "\n",
    "# BsmtExposure fillna using its distribution\n",
    "test_data.BsmtExposure[test_data.BsmtExposure.isna()] = test_data.BsmtExposure.mode()[0]\n",
    "\n",
    "# BsmtFinType1 fillna using its distribution\n",
    "t = [*['GLQ']*42]\n",
    "t.extend(['Unf']*38)\n",
    "temp = r.sample(t,42)\n",
    "test_data.BsmtFinType1[test_data.BsmtFinType1.isna()] = temp\n",
    "\n",
    "# BsmtFinType2 fillna using its distribution\n",
    "test_data.BsmtFinType2[test_data.BsmtFinType2.isna()] = test_data.BsmtFinType2.mode()[0]\n",
    "\n",
    "# Electrical fillna using its distribution\n",
    "test_data.KitchenQual[test_data.KitchenQual.isna()] = test_data.KitchenQual.mode()[0]\n",
    "\n",
    "# GarageType fillna using its distribution\n",
    "t = [*['Attchd']*80]\n",
    "t.extend(['Detchd']*6)\n",
    "temp = r.sample(t,76)\n",
    "test_data.GarageType[test_data.GarageType.isna()] = temp\n",
    "\n",
    "# GarageFinish fillna using its distribution\n",
    "t = [*['Unf']*40]\n",
    "t.extend(['RFn']*30)\n",
    "t.extend(['Fin']*28)\n",
    "temp = r.sample(t,78)\n",
    "test_data.GarageFinish[test_data.GarageFinish.isna()] = temp\n",
    "\n",
    "#SaleType fillna using its distribution\n",
    "test_data.SaleType[test_data.SaleType.isna()] = test_data.SaleType.mode()[0]\n",
    "\n",
    "split = round(raw_data.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = raw_data[i].value_counts()\n",
    "    raw_data[i][raw_data[i].isin(count[count < split].index)] = 'allmix'\n",
    "    \n",
    "    # value which are only 2% of all dataset unique will assign in a new column called allmix\n",
    "split = round(test_data.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = test_data[i].value_counts()\n",
    "    test_data[i][test_data[i].isin(count[count < split].index)] = 'allmix'\n",
    "\n",
    "\n",
    "test_data.Exterior1st[test_data.Exterior1st == 'WdShing'] = 'allmix'\n",
    "test_data.BsmtFinType2[test_data.BsmtFinType2 == 'ALQ'] = 'allmix'\n",
    "test_data.BldgType[test_data.BldgType == '2fmCon'] = 'allmix'\n",
    "\n",
    "\n",
    "# Concatinating One hot feature with remaining\n",
    "raw_data = pd.concat([raw_data,pd.get_dummies(raw_data[object_features_2])],axis=1)\n",
    "raw_data.drop(object_features_2, inplace=True,axis=1) # drop these feature because we hot encode these values\n",
    "raw_data.drop(drop_obj_feat, inplace=True,axis=1) # drop these feature because these are not important \n",
    "\n",
    "test_data = pd.concat([test_data,pd.get_dummies(test_data[object_features_2])],axis=1)\n",
    "test_data.drop(object_features_2, inplace=True,axis=1) # drop these feature because we hot encode these values\n",
    "test_data.drop(drop_obj_feat, inplace=True,axis=1) # drop these feature because these are not important \n",
    "\n",
    "\n",
    "raw_data.shape , raw_label.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([raw_data,test_data], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()\n",
    "\n",
    "for i in object_features_2:\n",
    "    all_data[i] = lbe.fit_transform(all_data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2838, 191)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([all_data[int_features], pd.get_dummies(all_data[object_features_2])],axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2838, 31)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[object_features_2].shape\n",
    "all_data[int_features].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv('raw_data.csv',index=False)\n",
    "test_data.to_csv('test_data.csv',index=False)\n",
    "pd.DataFrame({'SalePrice':raw_label}).to_csv('raw_label.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.Exterior1st[test_data.Exterior1st == 'WdShing'] = 'allmix'\n",
    "test_data.BsmtFinType2[test_data.BsmtFinType2 == 'ALQ'] = 'allmix'\n",
    "test_data.BldgType[test_data.BldgType == '2fmCon'] = 'allmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.get_dummies(raw_data[object_features_2]).columns.tolist()  \n",
    "b = pd.get_dummies(test_data[object_features_2]).columns.tolist()\n",
    "for i in range(114):\n",
    "    print(a[i],b[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_label = pd.DataFrame(raw_label)\n",
    "raw_label.columns = ['SalePrice']\n",
    "raw_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Datasets\n",
    "raw_data = pd.read_csv('F:/Books/Machine Learning/DataSets/house-prices-advanced-regression-technique/train.csv') \n",
    "test_data = pd.read_csv('F:/Books/Machine Learning/DataSets/house-prices-advanced-regression-technique/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract different columns between testdata and traindata\n",
    "set(raw_data.columns).difference(test_data.columns) # this is a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape of datasets\n",
    "print(raw_data.shape)\n",
    "print(test_data.shape)\n",
    "print(raw_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about data\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checking total no. of object,int,float type features\n",
    "object_features = raw_data.dtypes[raw_data.dtypes == 'object'].keys()\n",
    "int_features    = raw_data.dtypes[raw_data.dtypes == 'int64'].keys()\n",
    "float_features  = raw_data.dtypes[raw_data.dtypes == 'float64'].keys()\n",
    "\n",
    "#raw_data.select_dtypes(include=[object]).columns \n",
    "\n",
    "print(len(object_features)) # 43\n",
    "print(len(int_features))    # 35\n",
    "print(len(float_features))  #  3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Exploration, Engineering and Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dealing with Object Data_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(raw_data[object_features].isnull())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(test_data[object_features].isnull())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both test and train data has approx. same distribution i.e both come from same datasets\n",
    "plt.figure(figsize=(15,150))\n",
    "for j,i in enumerate(object_features):\n",
    "    temp = j*2+1\n",
    "    plt.subplot(43,2,temp) \n",
    "    sns.countplot(raw_data[i])  \n",
    "    plt.subplot(43,2,temp+1)\n",
    "    sns.countplot(test_data[i])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in object_features:\n",
    "    print(raw_data[i].value_counts())\n",
    "    print(test_data[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "##### Dropping  Object Feature ######\n",
    "#####################################\n",
    "# Street     :: 99% are same (these values dont provide us information because all values are same)\n",
    "# Alley      :: only 91 values remaining are null. So we can drop this column\n",
    "# Utilities  :: 99.99% are same\n",
    "# LandSlope  :: same values most\n",
    "# Condition2 :: same values most\n",
    "# RoofMatl   :: same values most\n",
    "# Heating    :: same values most\n",
    "# BsmtCond   :: same values most\n",
    "# PoolQC     :: null values most\n",
    "# MiscFeature:: null values most\n",
    "# HeatingQC  :: mostly yes\n",
    "# Functional :: not varies\n",
    "# GarageQual :: not varies ## take it\n",
    "# GarageCond :: same value most\n",
    "# PavedDrive :: null value most\n",
    "# MiscFeature:: mostly are null\n",
    "# ExterCond  :: same value most\n",
    "# CentralAir :: not varies\n",
    "# Fence      :: not varies\n",
    "# MasVnrType :: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_obj_feat = ['Street','Alley','Utilities','LandSlope','Condition2','RoofMatl','Heating',\n",
    "                'BsmtCond','PoolQC','MiscFeature','HeatingQC','Functional','GarageQual','GarageCond',\n",
    "                'PavedDrive','MiscFeature','ExterCond','CentralAir','Fence','FireplaceQu','MasVnrType','Electrical']\n",
    "\n",
    "object_features_2 = object_features.drop(drop_obj_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Dealing with missing values in Object DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "## Dealing with missing values of Train_data Object_datatype features ##\n",
    "########################################################################\n",
    "\n",
    "# finding all null valued columns \n",
    "temp = raw_data[object_features_2].isna().sum()[raw_data[object_features_2].isna().sum()!=0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in temp.index:\n",
    "    sns.countplot(raw_data[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BsmtQual fillna using its distribution\n",
    "t = [*['TA']*25]\n",
    "t.extend(['Gd']*20)\n",
    "temp = r.sample(t,36)\n",
    "raw_data.BsmtQual[raw_data.BsmtQual.isna()] = temp\n",
    "\n",
    "# BsmtExposure fillna using its distribution\n",
    "raw_data.BsmtExposure[raw_data.BsmtExposure.isna()] = raw_data.BsmtExposure.mode()[0]\n",
    "\n",
    "# BsmtFinType1 fillna using its distribution\n",
    "t = [*['GLQ']*21]\n",
    "t.extend(['Unf']*20)\n",
    "temp = r.sample(t,36)\n",
    "raw_data.BsmtFinType1[raw_data.BsmtFinType1.isna()] = temp\n",
    "\n",
    "# BsmtFinType2 fillna using its distribution\n",
    "raw_data.BsmtFinType2[raw_data.BsmtFinType2.isna()] = raw_data.BsmtFinType2.mode()[0]\n",
    "\n",
    "# Electrical fillna using its distribution\n",
    "raw_data.Electrical[raw_data.Electrical.isna()] = raw_data.Electrical.mode()[0]\n",
    "\n",
    "# GarageType fillna using its distribution\n",
    "t = [*['Attchd']*80]\n",
    "t.extend(['Detchd']*8)\n",
    "temp = r.sample(t,71)\n",
    "raw_data.GarageType[raw_data.GarageType.isna()] = temp\n",
    "\n",
    "# GarageFinish fillna using its distribution\n",
    "t = [*['Unf']*35]\n",
    "t.extend(['RFn']*30)\n",
    "t.extend(['Fin']*25)\n",
    "temp = r.sample(t,71)\n",
    "raw_data.GarageFinish[raw_data.GarageFinish.isna()] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[object_features_2].isna().sum()[raw_data[object_features_2].isna().sum()!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "## Dealing with missing values of Test_data Object_datatype features ##\n",
    "########################################################################\n",
    "\n",
    "# finding all null valued columns\n",
    "temp = test_data[object_features_2].isna().sum()[test_data[object_features_2].isna().sum() != 0 ]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in temp.index:\n",
    "    sns.countplot(test_data[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSZoning fillna using its distribution\n",
    "test_data.MSZoning[test_data.MSZoning.isna()] = test_data.MSZoning.mode()[0]\n",
    "\n",
    "# Exterior1st fillna using its distribution\n",
    "test_data.Exterior1st[test_data.Exterior1st.isna()] = test_data.Exterior1st.mode()[0]\n",
    "\n",
    "# Exterior2nd  fillna using its distribution\n",
    "test_data.Exterior2nd[test_data.Exterior2nd.isna()] = test_data.Exterior2nd.mode()[0]\n",
    "\n",
    "# BsmtQual fillna using its distribution\n",
    "t = [*['TA']*30]\n",
    "t.extend(['Gd']*25)\n",
    "temp = r.sample(t,44)\n",
    "test_data.BsmtQual[test_data.BsmtQual.isna()] = temp\n",
    "\n",
    "# BsmtExposure fillna using its distribution\n",
    "test_data.BsmtExposure[test_data.BsmtExposure.isna()] = test_data.BsmtExposure.mode()[0]\n",
    "\n",
    "# BsmtFinType1 fillna using its distribution\n",
    "t = [*['GLQ']*42]\n",
    "t.extend(['Unf']*38)\n",
    "temp = r.sample(t,42)\n",
    "test_data.BsmtFinType1[test_data.BsmtFinType1.isna()] = temp\n",
    "\n",
    "# BsmtFinType2 fillna using its distribution\n",
    "test_data.BsmtFinType2[test_data.BsmtFinType2.isna()] = test_data.BsmtFinType2.mode()[0]\n",
    "\n",
    "# Electrical fillna using its distribution\n",
    "test_data.KitchenQual[test_data.KitchenQual.isna()] = test_data.KitchenQual.mode()[0]\n",
    "\n",
    "# GarageType fillna using its distribution\n",
    "t = [*['Attchd']*80]\n",
    "t.extend(['Detchd']*6)\n",
    "temp = r.sample(t,76)\n",
    "test_data.GarageType[test_data.GarageType.isna()] = temp\n",
    "\n",
    "# GarageFinish fillna using its distribution\n",
    "t = [*['Unf']*40]\n",
    "t.extend(['RFn']*30)\n",
    "t.extend(['Fin']*28)\n",
    "temp = r.sample(t,78)\n",
    "test_data.GarageFinish[test_data.GarageFinish.isna()] = temp\n",
    "\n",
    "#SaleType fillna using its distribution\n",
    "test_data.SaleType[test_data.SaleType.isna()] = test_data.SaleType.mode()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test_data[object_features_2].isna().sum()[test_data[object_features_2].isna().sum() != 0 ]\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Masking the values of Object DataTypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.shape)   #0001   1460\n",
    "print(test_data.shape)  # 1461  2919\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both test and train data has approx. same distribution i.e both come from same datasets\n",
    "plt.figure(figsize=(15,150))\n",
    "for j,i in enumerate(object_features_2):\n",
    "    temp = j*2+1\n",
    "    plt.subplot(43,2,temp) \n",
    "    sns.countplot(raw_data[i])  \n",
    "    plt.subplot(43,2,temp+1)\n",
    "    sns.countplot(test_data[i])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to find unique value between train and test dataset of object datatype \n",
    "for i in object_features_2:\n",
    "    te = set(raw_data[i].value_counts().index).difference(test_data[i].value_counts().index)\n",
    "    if te:\n",
    "        print(i,'=',te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only these features contains different unique values\n",
    "plt.figure(figsize=(15,150))\n",
    "for j,i in enumerate(['HouseStyle','Exterior1st','Exterior2nd']):\n",
    "    temp = j*2+1\n",
    "    plt.subplot(43,2,temp) \n",
    "    sns.countplot(raw_data[i])  \n",
    "    plt.subplot(43,2,temp+1)\n",
    "    sns.countplot(test_data[i])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = round(raw_data.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = raw_data[i].value_counts()\n",
    "    raw_data[i][raw_data[i].isin(count[count < split].index)] = 'allmix'\n",
    "    \n",
    "    # value which are only 2% of all dataset unique will assign in a new column called allmix\n",
    "split = round(test_data.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = test_data[i].value_counts()\n",
    "    test_data[i][test_data[i].isin(count[count < split].index)] = 'allmix'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([raw_data,test_data],axis=0) \n",
    "raw_data2 = raw_data.copy()\n",
    "test_data2 = test_data.copy()\n",
    "\n",
    "split = round(new_df.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = new_df[i].value_counts()\n",
    "    new_df[i][new_df[i].isin(count[count < split].index)] = 'allmix'\n",
    "    \n",
    "    # value which are only 2% of all dataset unique will assign in a new column called allmix\n",
    "split = round(raw_data2.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = raw_data2[i].value_counts()\n",
    "    raw_data2[i][raw_data2[i].isin(count[count < split].index)] = 'allmix'\n",
    "    \n",
    "    # value which are only 2% of all dataset unique will assign in a new column called allmix\n",
    "split = round(test_data2.shape[0] * .02)  \n",
    "for i in object_features_2:\n",
    "    count = test_data2[i].value_counts()\n",
    "    test_data2[i][test_data2[i].isin(count[count < split].index)] = 'allmix'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.Exterior1st[test_data.Exterior1st == 'WdShing'] = 'allmix'\n",
    "test_data.BsmtFinType2[test_data.BsmtFinType2 == 'ALQ'] = 'allmix'\n",
    "\n",
    "print(pd.get_dummies(raw_data[object_features_2]).shape)  \n",
    "print(pd.get_dummies(test_data[object_features_2]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatinating One hot feature with remaining\n",
    "raw_data = pd.concat([raw_data,pd.get_dummies(raw_data[object_features_2])],axis=1)\n",
    "raw_data.drop(object_features_2, inplace=True,axis=1) # drop these feature because we hot encode these values\n",
    "raw_data.drop(drop_obj_feat, inplace=True,axis=1) # drop these feature because these are not important \n",
    "\n",
    "test_data = pd.concat([test_data,pd.get_dummies(test_data[object_features_2])],axis=1)\n",
    "test_data.drop(object_features_2, inplace=True,axis=1) # drop these feature because we hot encode these values\n",
    "test_data.drop(drop_obj_feat, inplace=True,axis=1) # drop these feature because these are not important \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = raw_data.columns.tolist()\n",
    "b = test_data.columns.tolist()\n",
    "for i in range(148):\n",
    "    if a[i] != b[i]:\n",
    "        print(a[i],b[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dealing with int,float DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[int_features].hist(bins='auto',figsize=(15,35),layout=(13,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Dealing with Missing, Outlier of IntDataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.select_dtypes('int64').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Int datatype features not contains any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.select_dtypes('int64').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect Int datatype outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#raw_data.LotArea[raw_data.LotArea > 35000] # drop these 18 observations\n",
    "#raw_data.OverallQual[raw_data.OverallQual < 2]  # 375 , 533\n",
    "## OverallCond ## seem to be important\n",
    "## OverallQual ## seem to be important\n",
    "## raw_data.OverallCond[raw_data.OverallCond < 2]  # 375\n",
    "## yearbuild       ok\n",
    "## yearremodadd    ok\n",
    "#raw_data.BsmtFinSF1[raw_data.BsmtFinSF1 > 2500]   # 1298\n",
    "#raw_data.BsmtFinSF2.value_counts() DROP\n",
    "## BsmtfinUnfSF    ok\n",
    "#raw_data.TotalBsmtSF[raw_data.TotalBsmtSF > 3500]  # 1298\n",
    "#raw_data['1stFlrSF'][raw_data['1stFlrSF'] > 3500]  # 1298\n",
    "# 2ndflrsf         ok\n",
    "#raw_data.LowQualFinSF[raw_data.LowQualFinSF > 500]  # 88,170,185,635,1009\n",
    "#raw_data.GrLivArea[raw_data.GrLivArea > 5000]       # 1298\n",
    "#raw_data.BsmtFullBath[raw_data.BsmtFullBath > 2]  # 738\n",
    "#BsmthalfBath   drop\n",
    "#raw_data.FullBath[raw_data.FullBath < 1] # 9 observations\n",
    "# Halfbath      ok\n",
    "#raw_data.BedroomAbvGr[raw_data.BedroomAbvGr > 6] # 635 \n",
    "#raw_data.KitchenAbvGr[raw_data.KitchenAbvGr > 2] # 48, 809\n",
    "# fireplace     ok\n",
    "# garage cars   ok\n",
    "# garage ares   ok\n",
    "#raw_data.WoodDeckSF[raw_data.WoodDeckSF > 600] # 6 observations\n",
    "#raw_data.OpenPorchSF[raw_data.OpenPorchSF > 400] # 5 observations\n",
    "#raw_data.EnclosedPorch[raw_data.EnclosedPorch > 300] # 5 observations\n",
    "# 3SnPorsch  drop\n",
    "#raw_data.ScreenPorch[raw_data.ScreenPorch > 350]  # 6 observations\n",
    "# poolarea   drop\n",
    "#raw_data.MiscVal[raw_data.MiscVal > 2100]  # 346,705,1230,1457\n",
    "# MoSold    ok\n",
    "# YrSold    ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution and detect outliers\n",
    "for i in raw_data.select_dtypes('int64').drop('Id',axis=1):\n",
    "    sns.distplot(raw_data[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outliers = []\n",
    "drop_int_feat = []\n",
    "outliers.extend(raw_data.LotArea[raw_data.LotArea > 30000].index.tolist()) \n",
    "outliers.extend(raw_data.OverallQual[raw_data.OverallQual < 3].index.tolist()) \n",
    "outliers.extend(raw_data.OverallCond[raw_data.OverallCond < 3].index.tolist())  \n",
    "outliers.extend(raw_data.BsmtFinSF1[raw_data.BsmtFinSF1 > 2000].index.tolist()) \n",
    "outliers.extend(raw_data.LowQualFinSF[raw_data.LowQualFinSF > 500].index.tolist()) \n",
    "outliers.extend(raw_data.BsmtFullBath[raw_data.BsmtFullBath > 2].index.tolist())\n",
    "outliers.extend(raw_data.TotalBsmtSF[raw_data.TotalBsmtSF > 2600].index.tolist())\n",
    "outliers.extend(raw_data['1stFlrSF'][raw_data['1stFlrSF'] > 2650 ].index.tolist())\n",
    "outliers.extend(raw_data['2ndFlrSF'][raw_data['2ndFlrSF'] > 1600 ].index.tolist())\n",
    "outliers.extend(raw_data.BsmtUnfSF[raw_data.BsmtUnfSF > 2200].index.tolist())\n",
    "outliers.extend(raw_data.FullBath[raw_data.FullBath < 1].index.tolist()) \n",
    "outliers.extend(raw_data.BedroomAbvGr[raw_data.BedroomAbvGr > 6].index.tolist())\n",
    "outliers.extend(raw_data.BedroomAbvGr[raw_data.BedroomAbvGr < 1].index.tolist())\n",
    "outliers.extend(raw_data.KitchenAbvGr[raw_data.KitchenAbvGr > 2].index.tolist())\n",
    "outliers.extend(raw_data.GarageArea[raw_data.GarageArea > 1250].index.tolist())\n",
    "outliers.extend(raw_data.WoodDeckSF[raw_data.WoodDeckSF > 600].index.tolist()) \n",
    "outliers.extend(raw_data.OpenPorchSF[raw_data.OpenPorchSF > 400].index.tolist()) \n",
    "outliers.extend(raw_data.EnclosedPorch[raw_data.EnclosedPorch > 300].index.tolist()) \n",
    "outliers.extend(raw_data.ScreenPorch[raw_data.ScreenPorch > 350].index.tolist())  \n",
    "outliers.extend(raw_data.SalePrice[raw_data.SalePrice > 560000].index.tolist())  \n",
    "\n",
    "drop_int_feat.append('BsmtHalfBath')\n",
    "drop_int_feat.append('BsmtFinSF2')\n",
    "drop_int_feat.append('3SsnPorch')\n",
    "drop_int_feat.append('PoolArea')\n",
    "drop_int_feat.append('MiscVal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Dealing with Missing, Outlier values of Float DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.select_dtypes('float64').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Float datatype features contains missing values\n",
    "\n",
    "sns.heatmap(raw_data.select_dtypes('float64').isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raw_data.select_dtypes('float64').columns:\n",
    "    sns.distplot(raw_data[i].dropna())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill missing values after removing outliers\n",
    "\n",
    "outlier = raw_data.LotFrontage[raw_data.LotFrontage > 200].index.tolist()\n",
    "outliers.extend(outlier)\n",
    "\n",
    "m = raw_data.LotFrontage.drop(outlier).mean()\n",
    "s = raw_data.LotFrontage.drop(outlier).std()\n",
    "randoms = np.random.normal(m , s/1.5 , raw_data.LotFrontage.isna().sum())\n",
    "raw_data.LotFrontage[raw_data.LotFrontage.isna()] = randoms\n",
    "\n",
    "\n",
    "outlier = raw_data.MasVnrArea[raw_data.MasVnrArea > 1100].index.tolist()\n",
    "outliers.extend(outlier)\n",
    "m = raw_data.MasVnrArea.drop(outlier).mean()\n",
    "raw_data.MasVnrArea.fillna(m,inplace=True)\n",
    "\n",
    "\n",
    "x = np.random.normal(round(raw_data.GarageYrBlt.mean()),\n",
    "                        round(raw_data.GarageYrBlt.std()/2),\n",
    "                        raw_data.GarageYrBlt.isna().sum())\n",
    "raw_data.GarageYrBlt[raw_data.GarageYrBlt.isna()]  =  list(map(round,x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in raw_data.select_dtypes('float64').columns:\n",
    "    sns.distplot(raw_data[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "# Detect outlier using Interquartile rate\n",
    "\n",
    "def outlierDetect(df, no_of_feature_contains = 3):\n",
    "    '''pass pandas\"s dataframe'''\n",
    "    outlier_indices = []\n",
    "    for col in df:\n",
    "        q1 = np.percentile(df[col],25)\n",
    "        q3 = np.percentile(df[col],75)\n",
    "        # IQR\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        outlier_index = df[ (df[col] < lower) | (df[col] > upper)].index.tolist()\n",
    "        outlier_indices.extend(outlier_index)\n",
    "\n",
    "    counted_outliers = Counter(outlier_indices)    \n",
    "    indexs = [k for k,v in counted_outliers.items() if v > no_of_feature_contains] \n",
    "    return indexs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outliers.extend(outlierDetect(raw_data[int_features],no_of_feature_contains = 5))\n",
    "\n",
    "# Dropping all outliers \n",
    "raw_data.drop(set(outliers), inplace = True)\n",
    "\n",
    "# Seperate labels from dataset\n",
    "raw_label = raw_data.SalePrice\n",
    "raw_data.drop('SalePrice',axis=1,inplace=True)\n",
    "\n",
    "# Droping int features\n",
    "raw_data.drop(drop_int_feat,axis=1,inplace=True)\n",
    "test_data.drop(drop_int_feat,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "int_features = int_features.append(raw_data.select_dtypes('float64').columns)\n",
    "int_features = int_features.drop(drop_int_feat)\n",
    "int_features = int_features.drop('Id')\n",
    "int_features = int_features.drop('SalePrice')\n",
    "\n",
    "#uint8_features = raw_data.select_dtypes('uint8').columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float in test data contains outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing data\n",
    "temp = test_data.select_dtypes('float64').isna().sum()[test_data.select_dtypes('float64').isna().sum() !=0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outlier = test_data.LotFrontage[test_data.LotFrontage > 150].index.tolist()\n",
    "m = test_data.LotFrontage.drop(outlier).mean()                                      \n",
    "s = test_data.LotFrontage.drop(outlier).std()\n",
    "randoms = np.random.normal(m , s/1.5 , test_data.LotFrontage.isna().sum())\n",
    "test_data.LotFrontage[test_data.LotFrontage.isna()] = randoms\n",
    "\n",
    "test_data.MasVnrArea.fillna(0,inplace=True)\n",
    "test_data.BsmtFinSF1.fillna(0,inplace=True)\n",
    "test_data.BsmtUnfSF.fillna(0,inplace=True)\n",
    "test_data.TotalBsmtSF.fillna(0,inplace=True)\n",
    "test_data.BsmtFullBath.fillna(0,inplace=True)\n",
    "test_data.GarageCars.fillna(2,inplace=True)\n",
    "test_data.GarageArea.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "x = np.random.normal(round(test_data.GarageYrBlt.mean()),\n",
    "                        round(test_data.GarageYrBlt.std()/2),\n",
    "                        test_data.GarageYrBlt.isna().sum())\n",
    "test_data.GarageYrBlt[test_data.GarageYrBlt.isna()]  =  list(map(round,x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = raw_data[int_features].corr()\n",
    "plt.figure(figsize=(16,16))\n",
    "sns.heatmap(corr, cbar = True,  square = True, \n",
    "            annot=True, fmt= '.2f',annot_kws={'size': 5},\n",
    "           xticklabels= int_features, yticklabels= int_features, \n",
    "            alpha = 0.7,   cmap= 'coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.shape)\n",
    "print(raw_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost              import XGBRegressor\n",
    "from sklearn.svm          import SVR\n",
    "from sklearn.ensemble     import RandomForestRegressor\n",
    "from sklearn.ensemble     import AdaBoostRegressor\n",
    "from sklearn.ensemble     import ExtraTreesRegressor \n",
    "from sklearn.ensemble     import GradientBoostingRegressor\n",
    "from sklearn.neighbors    import KNeighborsRegressor\n",
    "from sklearn.metrics      import r2_score, accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline     import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(raw_data.iloc[:,1:],\n",
    "                                                    raw_label,\n",
    "                                                   test_size=.25,\n",
    "                                                   random_state= 42,\n",
    "                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_features = int_features.drop('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "new_x_train = sc.fit_transform(x_train[int_features])\n",
    "new_x_test = sc.transform(x_test[int_features])\n",
    "\n",
    "x_train[int_features] = new_x_train\n",
    "x_test[int_features] = new_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(tree_method='gpu_hist',objective='reg:squarederror')\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "n_estimators = 200\n",
    "seed = 42\n",
    "pipelines.append( ( 'SVR' , Pipeline( [('SVC',SVR()) ]) ) ) \n",
    "\n",
    "pipelines.append( ( 'KNN' , Pipeline( [('KNN',KNeighborsRegressor()) ]) ) )\n",
    "\n",
    "pipelines.append( ( 'RF'  , Pipeline( [('RF', RandomForestRegressor(random_state=seed, n_estimators=n_estimators)) ]) ) )\n",
    "\n",
    "pipelines.append( ( 'Ada' , Pipeline( [('Ada',AdaBoostRegressor(random_state=seed    , n_estimators=n_estimators)) ]) ) )\n",
    "\n",
    "pipelines.append( ( 'ET'  , Pipeline( [('ET', ExtraTreesRegressor(random_state=seed  , n_estimators=n_estimators)) ]) ) )\n",
    "\n",
    "pipelines.append( ( 'GB'  , Pipeline( [('GB', GradientBoostingRegressor(random_state=seed)) ]) ) )\n",
    "\n",
    "pipelines.append( ( 'XGB'  , Pipeline( [('XGB', XGBRegressor(random_state=seed)) ]) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, names  = [], []\n",
    "for name, model in pipelines:\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv = 5 ) \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    \n",
    "fig = plt.figure(figsize=(12,8))    \n",
    "fig.suptitle(\"Algorithms comparison\")\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.DataFrame(results,index=names,columns=['CV1','CV2','CV3','CV4','CV5'])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor()\n",
    "gbm.fit(x_train,y_train)\n",
    "gbm.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbm = XGBRegressor()\n",
    "xgbm.fit(x_train,y_train)\n",
    "xgbm.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbm.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=3, random_state=7)\n",
    "result = cross_val_score(lr, X, y, cv=kfold, scoring='accuracy')\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "params = {'learning_rate':[0.1,0.08,0.05,0.01,0.001],\n",
    "         'gamma':[0.01,0.1,0.3,0.5,1,1.5,2],\n",
    "         'max_depth':[2,4,7,10],\n",
    "         'colsample_bytree':[0.3,0.6,0.8,1],\n",
    "         \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "         \"reg_alpha\": [0, 0.5, 1],\n",
    "         \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "         \"min_child_weight\": [1, 3, 5, 7],\n",
    "         \"n_estimators\": [100, 250, 500, 1000]}\n",
    "\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = params,\n",
    "                             cv = 7, verbose = 3, random_state = 40, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb_rscv.fit(x_train,y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (model_xgb.best_score_, model_xgb.best_params_))\n",
    "\n",
    "print(model_xgb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 0.3,  # this I get after random search\n",
    "         'gamma': 1.5,\n",
    "         'learning_rate': 0.01,\n",
    "         'max_depth': 7,\n",
    "         'min_child_weight': 1,\n",
    "         'n_estimators': 1000,\n",
    "         'reg_alpha': 0,\n",
    "         'reg_lambda': 1.5,\n",
    "         'subsample': 0.4}\n",
    "\n",
    "model_xgb = XGBRegressor( tree_method='gpu_hist',objective='reg:squarederror',**best_params)\n",
    "model_xgb.fit(x_train,y_train)\n",
    "model_xgb.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_xgb.predict(x_test)\n",
    "np.sqrt(mean_squared_log_error( y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.to_csv('new_raw.csv')\n",
    "#test_data.to_csv('new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_test = sc.transform(test_data[int_features])\n",
    "test_data[int_features] = new_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(iterations=1000, task_type=\"GPU\",devices='0:1')\n",
    "model.fit(x_train,y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "np.sqrt(mean_squared_log_error( y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval_metrics(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = raw_data.iloc[:,(np.argsort(model_xgb.feature_importances_))[:80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new_df,\n",
    "                                                    raw_label,\n",
    "                                                   test_size=.25,\n",
    "                                                   random_state= 42,\n",
    "                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 0.3,  # this I get after random search\n",
    "         'gamma': 1.5,\n",
    "         'learning_rate': 0.01,\n",
    "         'max_depth': 7,\n",
    "         'min_child_weight': 1,\n",
    "         'n_estimators': 1000,\n",
    "         'reg_alpha': 0,\n",
    "         'reg_lambda': 1.5,\n",
    "         'subsample': 0.4}\n",
    "\n",
    "model_xgb2 = XGBRegressor( tree_method='gpu_hist',objective='reg:squarederror',**best_params)\n",
    "model_xgb2.fit(new_df,y_train)\n",
    "model_xgb2.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [mean_squared_error(y_test,y_pred) for y_pred in model_xgb.staged_predict(x_test)]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
