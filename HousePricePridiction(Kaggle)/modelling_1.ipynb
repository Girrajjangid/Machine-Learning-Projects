{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import numpy       as np\n",
    "import pandas      as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn     as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import random as r\n",
    "from xgboost              import XGBRegressor\n",
    "from catboost             import CatBoostRegressor\n",
    "from sklearn.svm          import SVR\n",
    "from sklearn.ensemble     import RandomForestRegressor\n",
    "from sklearn.ensemble     import AdaBoostRegressor\n",
    "from sklearn.ensemble     import ExtraTreesRegressor \n",
    "from sklearn.ensemble     import GradientBoostingRegressor\n",
    "from sklearn.neighbors    import KNeighborsRegressor\n",
    "from sklearn.metrics      import r2_score, accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline     import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import plot_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sam = pd.read_csv('F:/Books/Machine Learning/DataSets/house-prices-advanced-regression-technique/sample_submission.csv')\n",
    "\n",
    "raw_data = pd.read_csv('raw_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "raw_label = pd.read_csv('raw_label.csv')\n",
    "raw_data.drop(['Id'],axis=1,inplace=True)\n",
    "test_data.drop(['Id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1379, 145), (1459, 145), (1379, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape , test_data.shape , raw_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2 = raw_data.copy()\n",
    "for c in raw_data.select_dtypes('object').columns.tolist():\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(raw_data2[c].values)) \n",
    "    raw_data2[c] = lbl.transform(list(raw_data2[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape all_data: {}'.format(raw_data2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(raw_data,\n",
    "                                                    raw_label,\n",
    "                                                   test_size=.25,\n",
    "                                                   random_state= 42,\n",
    "                                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sam.SalePrice = y_pred\n",
    "sub_sam.to_csv('submission4.csv',index=False,float_format='%.6f')\n",
    "pd.read_csv('submission4.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2['TotalSF'] = raw_data2['TotalBsmtSF'] + raw_data2['1stFlrSF'] + raw_data2['2ndFlrSF']\n",
    "raw_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "params = {'learning_rate':[0.1,0.08,0.05,0.01,0.001],\n",
    "         'gamma':[0.01,0.1,0.3,0.5,1,1.5,2],\n",
    "         'max_depth':[2,4,7,10],\n",
    "         'colsample_bytree':[0.3,0.6,0.8,1],\n",
    "         \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "         \"reg_alpha\": [0, 0.5, 1],\n",
    "         \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "         \"min_child_weight\": [1, 3, 5, 7],\n",
    "         \"n_estimators\": [100, 250, 500, 1000]}\n",
    "\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = params,\n",
    "                             cv = 7, verbose = 3, random_state = 40, n_jobs=-1)\n",
    "\n",
    "model_xgb = xgb_rscv.fit(x_train,y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (model_xgb.best_score_, model_xgb.best_params_))\n",
    "\n",
    "print(model_xgb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impp = [101, 107,  23, 129,   3, 126,  16,  14,  99, 136, 138, 110, 100,\n",
    "            143,  24, 102, 132, 105,  21,  10,  11,  33,  17, 109, 134,   8,\n",
    "            127,  12,  19,  50, 128,   5,  32,  22,  59,  69,  20, 117, 139,\n",
    "             58,  76, 112, 144, 133,  15,  34,   7, 6]\n",
    "# 70 features\n",
    "feat_imp = pd.Series(model_xgb.feature_importances_).sort_values(ascending=False)\n",
    "imp = feat_imp[:60].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb2 = XGBRegressor(objective='reg:squarederror',**best_params)\n",
    "model_xgb2.fit(x_train.iloc[:,imp_gbm],y_train)\n",
    "print(model_xgb2.score(x_test.iloc[:,imp_gbm],y_test))\n",
    "y_pred = model_xgb2.predict(x_test.iloc[:,imp_gbm])\n",
    "np.sqrt(mean_squared_log_error( y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb2 = XGBRegressor(objective='reg:squarederror',**best_params)\n",
    "model_xgb2.fit(raw_data.iloc[:,imp_gbm],raw_label)\n",
    "y_pred = model_xgb2.predict(test_data.iloc[:,imp_gbm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
    "\n",
    "gbm_model = GradientBoostingRegressor(**params)\n",
    "\n",
    "gbm_model.fit(x_train,y_train)\n",
    "\n",
    "gbm_model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm_model.predict(x_test)\n",
    "errors = [mean_squared_error(y_test,y_pred) for y_pred in gbm_model.staged_predict(x_test)]\n",
    "np.sqrt(mean_squared_log_error(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(gbm_model.feature_importances_).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_gbm = feat_imp[:52].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CatBoostRegressor(n_estimators=1000, # use large n_estimators deliberately to make use of the early stopping\n",
    "                         one_hot_max_size = 2,\n",
    "                         loss_function='Logloss',\n",
    "                         eval_metric='AUC',\n",
    "                         boosting_type='Ordered', # use permutations\n",
    "                         random_seed=2405, \n",
    "                         use_best_model=True,\n",
    "                         silent=True)\n",
    "params_space = [Real(0.01, 0.8, name='learning_rate'), \n",
    "                Integer(2, 10, name='max_depth'), \n",
    "                Real(0.5, 1.0, name='colsample_bylevel'), \n",
    "                Real(1.0, 16.0, name='scale_pos_weight'), \n",
    "                Real(0.0, 100, name='bagging_temperature'), \n",
    "                Real(0.0, 100, name='random_strength'), \n",
    "                Real(1.0, 100, name='reg_lambda')]\n",
    "one_cb_optimal_values = one_cb_optimizer.optimize(params_space, max_evals=40, n_random_starts=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 0.3,  # this I get after random search\n",
    "         'gamma': 1.5,\n",
    "         'learning_rate': 0.01,\n",
    "         'max_depth': 7,\n",
    "         'min_child_weight': 1,\n",
    "         'n_estimators': 1000,\n",
    "         'reg_alpha': 0,\n",
    "         'reg_lambda': 1.5,\n",
    "         'subsample': 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2 = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(raw_data.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, raw_data.values, raw_label.values.reshape(-1), scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.005, random_state=1))\n",
    "\n",
    "ENet  = make_pipeline(RobustScaler(), ElasticNet(alpha=0.005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "KRR   = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=250, learning_rate=0.1,\n",
    "                                   max_depth=3, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "best_params = {'colsample_bytree': 0.3,  # this I get after random search\n",
    "                'gamma': 1.5,\n",
    "                'learning_rate': 0.01,\n",
    "                'max_depth': 7,\n",
    "                'min_child_weight': 1,\n",
    "                'n_estimators': 1000,\n",
    "                'reg_alpha': 0,\n",
    "                'reg_lambda': 1.5,\n",
    "                'subsample': 0.4,\n",
    "                'random_state':7,\n",
    "                'nthread':-1}\n",
    "\n",
    "model_xgb = XGBRegressor(**best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple stacking average all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AveragingModels(models=(Pipeline(memory=None,\n",
       "                                 steps=[('robustscaler',\n",
       "                                         RobustScaler(copy=True,\n",
       "                                                      quantile_range=(25.0,\n",
       "                                                                      75.0),\n",
       "                                                      with_centering=True,\n",
       "                                                      with_scaling=True)),\n",
       "                                        ('elasticnet',\n",
       "                                         ElasticNet(alpha=0.005, copy_X=True,\n",
       "                                                    fit_intercept=True,\n",
       "                                                    l1_ratio=0.9, max_iter=1000,\n",
       "                                                    normalize=False,\n",
       "                                                    positive=False,\n",
       "                                                    precompute=False,\n",
       "                                                    random_state=3,\n",
       "                                                    selection='cyclic',\n",
       "                                                    tol=0.0001,\n",
       "                                                    warm...\n",
       "                                    kernel='polynomial', kernel_params=None),\n",
       "                        Pipeline(memory=None,\n",
       "                                 steps=[('robustscaler',\n",
       "                                         RobustScaler(copy=True,\n",
       "                                                      quantile_range=(25.0,\n",
       "                                                                      75.0),\n",
       "                                                      with_centering=True,\n",
       "                                                      with_scaling=True)),\n",
       "                                        ('lasso',\n",
       "                                         Lasso(alpha=0.005, copy_X=True,\n",
       "                                               fit_intercept=True,\n",
       "                                               max_iter=1000, normalize=False,\n",
       "                                               positive=False, precompute=False,\n",
       "                                               random_state=1,\n",
       "                                               selection='cyclic', tol=0.0001,\n",
       "                                               warm_start=False))],\n",
       "                                 verbose=False)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
    "averaged_models.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15765979808552907\n"
     ]
    }
   ],
   "source": [
    "y_pred = averaged_models.predict(x_test)\n",
    "print(np.sqrt(mean_squared_log_error(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58047.04916763, 29155.96980467, 25341.70674552, 30113.64694458,\n",
       "       38590.21831025])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = rmsle_cv(averaged_models)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Less simple Stacking : Adding a Meta-model\n",
    "\n",
    "In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n",
    "\n",
    "The procedure, for the training part, may be described as follows:\n",
    "\n",
    "Split the total training set into two disjoint sets (here train and .holdout )\n",
    "\n",
    "Train several base models on the first part (train)\n",
    "\n",
    "Test these base models on the second part (holdout)\n",
    "\n",
    "Use the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n",
    "\n",
    "The first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n",
    "\n",
    "So, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n",
    "\n",
    "For the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:52:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=1.5,\n",
       "             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=7, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "             n_jobs=1, nthread=-1, objective='reg:linear', random_state=7,\n",
       "             reg_alpha=0, reg_lambda=1.5, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=0.4, verbosity=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_xgb.fit(raw_data,raw_label)\n",
    "#print(model_xgb.score(x_test,y_test))\n",
    "#print(np.sqrt(mean_squared_log_error(y_test, model_xgb.predict(x_test) )))\n",
    "\n",
    "model_xgb2 = XGBRegressor(**best_params)\n",
    "model_xgb2.fit(raw_data,raw_label)\n",
    "#print(model_xgb2.score(x_test,y_test))\n",
    "#np.sqrt(mean_squared_log_error(y_test, model_xgb2.predict(x_test) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 0.3,  # this I get after random search\n",
    "                'gamma': 1.5,\n",
    "                'learning_rate': 0.01,\n",
    "                'max_depth': 7,\n",
    "                'min_child_weight': 1,\n",
    "                'n_estimators': 1000,\n",
    "                'reg_alpha': 0,\n",
    "                'reg_lambda': 1.5,\n",
    "                'subsample': 0.4,\n",
    "                'random_state':7,\n",
    "                'nthread':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9385425535413263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10477406965401104"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb3 = XGBRegressor(colsample_bytree=0.4, gamma=0.468, \n",
    "                             learning_rate=0.03, max_depth=5, \n",
    "                             min_child_weight=1.5817, n_estimators=1700,\n",
    "                             reg_alpha=0.2640, reg_lambda=1.0571,\n",
    "                             subsample=0.4813, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "model_xgb3.fit(raw_data,raw_label)\n",
    "#print(model_xgb3.score(x_test,y_test))\n",
    "y_pred3 = model_xgb3.predict(test_data)\n",
    "#np.sqrt(mean_squared_log_error(y_test, y_pred3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model_xgb.predict(test_data)\n",
    "y_pred2 = model_xgb2.predict(test_data)\n",
    "y_pred_avg = (y_pred1 + y_pred2 )/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [345, 1459]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-70d77d73e025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_log_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_avg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_log_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m    311\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 312\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    313\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \"\"\"\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\GirrajJangid\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [345, 1459]"
     ]
    }
   ],
   "source": [
    "ynp.sqrt(mean_squared_log_error(y_test, y_pred_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      233719.640625\n",
       "1      237233.546875\n",
       "2      379073.656250\n",
       "3      148414.734375\n",
       "4      258577.125000\n",
       "           ...      \n",
       "340    199766.250000\n",
       "341    153694.609375\n",
       "342    254866.687500\n",
       "343    204335.078125\n",
       "344     94667.601562\n",
       "Length: 345, dtype: float32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostRegressor(verbose=0)\n",
    "cat.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = cat.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_log_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingAveragedModels(base_models=(Pipeline(memory=None,\n",
       "                                             steps=[('robustscaler',\n",
       "                                                     RobustScaler(copy=True,\n",
       "                                                                  quantile_range=(25.0,\n",
       "                                                                                  75.0),\n",
       "                                                                  with_centering=True,\n",
       "                                                                  with_scaling=True)),\n",
       "                                                    ('elasticnet',\n",
       "                                                     ElasticNet(alpha=0.005,\n",
       "                                                                copy_X=True,\n",
       "                                                                fit_intercept=True,\n",
       "                                                                l1_ratio=0.9,\n",
       "                                                                max_iter=1000,\n",
       "                                                                normalize=False,\n",
       "                                                                positive=False,\n",
       "                                                                precompute=False,\n",
       "                                                                random_state=3,\n",
       "                                                                selection='cyclic',\n",
       "                                                                tol...\n",
       "                       meta_model=Pipeline(memory=None,\n",
       "                                           steps=[('robustscaler',\n",
       "                                                   RobustScaler(copy=True,\n",
       "                                                                quantile_range=(25.0,\n",
       "                                                                                75.0),\n",
       "                                                                with_centering=True,\n",
       "                                                                with_scaling=True)),\n",
       "                                                  ('lasso',\n",
       "                                                   Lasso(alpha=0.005,\n",
       "                                                         copy_X=True,\n",
       "                                                         fit_intercept=True,\n",
       "                                                         max_iter=1000,\n",
       "                                                         normalize=False,\n",
       "                                                         positive=False,\n",
       "                                                         precompute=False,\n",
       "                                                         random_state=1,\n",
       "                                                         selection='cyclic',\n",
       "                                                         tol=0.0001,\n",
       "                                                         warm_start=False))],\n",
       "                                           verbose=False),\n",
       "                       n_folds=5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_averaged_models.fit(x_train.values,y_train.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacked_averaged_models.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10553573319382772\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mean_squared_log_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(x_train.values, y_train.values.reshape(-1))\n",
    "stacked_train_pred = stacked_averaged_models.predict(x_train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(x_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmsle(y_train.values.reshape(-1), stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(raw_data2.values, raw_label.values.reshape(-1))\n",
    "\n",
    "stacked_train_pred = stacked_averaged_models.predict(raw_data2.values)\n",
    "\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test_data.values))\n",
    "\n",
    "print(rmsle(raw_label, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(raw_data2, raw_label)\n",
    "xgb_train_pred = model_xgb.predict(raw_data2)\n",
    "xgb_pred = np.expm1(model_xgb.predict(test_data))\n",
    "print(rmsle(raw_label, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSLE score on train data:')\n",
    "print(rmsle(raw_label,stacked_train_pred*0.80 +xgb_train_pred*0.20  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.80 + xgb_pred*0.20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = pd.read_csv('submission4.csv')\n",
    "s1 = pd.read_csv('submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>123115.555</td>\n",
       "      <td>119993.527990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>163208.080</td>\n",
       "      <td>160505.505505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>185206.860</td>\n",
       "      <td>186796.666810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>195114.440</td>\n",
       "      <td>196605.939658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>184240.880</td>\n",
       "      <td>192247.033913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SalePrice      SalePrice\n",
       "0  123115.555  119993.527990\n",
       "1  163208.080  160505.505505\n",
       "2  185206.860  186796.666810\n",
       "3  195114.440  196605.939658\n",
       "4  184240.880  192247.033913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([s1.SalePrice,s4.SalePrice],axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.SalePrice = y_pred_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.to_csv('submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
